{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "anBxr6PXP6A3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps in Text Preprocessing :\n",
        "\n",
        "Tokenization\n",
        "\n",
        "Lowercasing\n",
        "\n",
        "Removing Punctuation\n",
        "\n",
        "Removing Stop Words\n",
        "\n",
        "Stemming and Lemmatization\n",
        "\n",
        "Removing Special Characters and Numbers"
      ],
      "metadata": {
        "id": "rv1031_cTIhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSkSSkXZQGU5",
        "outputId": "cb581f75-76ac-4ab5-841b-df40945f6298"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"silence the voices in my head I pitched my poison now I'll drink it to the bitter end heyooo, hear me now, I'll take all the darkness by myself cos I'm ready to give whatever it takes standing at the edge of the fire I don't know how I ever survived I feel it burning under my skin and now I m back on my feet again\""
      ],
      "metadata": {
        "id": "vNVyOZ6VTZLb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQokxL_bVqrN",
        "outputId": "4676dd37-fb3e-4902-e15f-6d7906dc5ec9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "315"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "print(\"Word tokens : \",words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-QPD9wLVsvF",
        "outputId": "b7e6b338-adac-4b50-fd3e-37ae6a1f72a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens :  ['silence', 'the', 'voices', 'in', 'my', 'head', 'I', 'pitched', 'my', 'poison', 'now', 'I', \"'ll\", 'drink', 'it', 'to', 'the', 'bitter', 'end', 'heyooo', ',', 'hear', 'me', 'now', ',', 'I', \"'ll\", 'take', 'all', 'the', 'darkness', 'by', 'myself', 'cos', 'I', \"'m\", 'ready', 'to', 'give', 'whatever', 'it', 'takes', 'standing', 'at', 'the', 'edge', 'of', 'the', 'fire', 'I', 'do', \"n't\", 'know', 'how', 'I', 'ever', 'survived', 'I', 'feel', 'it', 'burning', 'under', 'my', 'skin', 'and', 'now', 'I', 'm', 'back', 'on', 'my', 'feet', 'again']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "tokenizer = Tokenizer(MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "UOdBiXJ4_L_g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2blI--fdCbVw",
        "outputId": "2914f0a4-6030-485e-f39e-9e52592dedc7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8], [2], [11], [1], [3], [17], [1], [], [4], [9], [1], [], [15], [5], [2], [17], [1], [8], [], [2], [3], [], [12], [13], [], [9], [1], [6], [7], [], [2], [], [23], [2], [4], [17], [9], [1], [7], [], [12], [13], [], [23], [5], [2], [8], [5], [3], [], [3], [5], [16], [], [2], [20], [11], [11], [], [7], [10], [2], [3], [14], [], [2], [4], [], [4], [5], [], [4], [9], [1], [], [21], [2], [4], [4], [1], [10], [], [1], [3], [7], [], [9], [1], [13], [5], [5], [5], [], [], [9], [1], [6], [10], [], [12], [1], [], [3], [5], [16], [], [], [2], [20], [11], [11], [], [4], [6], [14], [1], [], [6], [11], [11], [], [4], [9], [1], [], [7], [6], [10], [14], [3], [1], [8], [8], [], [21], [13], [], [12], [13], [8], [1], [11], [18], [], [17], [5], [8], [], [2], [20], [12], [], [10], [1], [6], [7], [13], [], [4], [5], [], [19], [2], [15], [1], [], [16], [9], [6], [4], [1], [15], [1], [10], [], [2], [4], [], [4], [6], [14], [1], [8], [], [8], [4], [6], [3], [7], [2], [3], [19], [], [6], [4], [], [4], [9], [1], [], [1], [7], [19], [1], [], [5], [18], [], [4], [9], [1], [], [18], [2], [10], [1], [], [2], [], [7], [5], [3], [20], [4], [], [14], [3], [5], [16], [], [9], [5], [16], [], [2], [], [1], [15], [1], [10], [], [8], [22], [10], [15], [2], [15], [1], [7], [], [2], [], [18], [1], [1], [11], [], [2], [4], [], [21], [22], [10], [3], [2], [3], [19], [], [22], [3], [7], [1], [10], [], [12], [13], [], [8], [14], [2], [3], [], [6], [3], [7], [], [3], [5], [16], [], [2], [], [12], [], [21], [6], [17], [14], [], [5], [3], [], [12], [13], [], [18], [1], [1], [4], [], [6], [19], [6], [2], [3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6Xn2a5hCsKj",
        "outputId": "04a3d988-9591-4bb0-8835-22e4e5731519"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'e': 1,\n",
              " 'i': 2,\n",
              " 'n': 3,\n",
              " 't': 4,\n",
              " 'o': 5,\n",
              " 'a': 6,\n",
              " 'd': 7,\n",
              " 's': 8,\n",
              " 'h': 9,\n",
              " 'r': 10,\n",
              " 'l': 11,\n",
              " 'm': 12,\n",
              " 'y': 13,\n",
              " 'k': 14,\n",
              " 'v': 15,\n",
              " 'w': 16,\n",
              " 'c': 17,\n",
              " 'f': 18,\n",
              " 'g': 19,\n",
              " \"'\": 20,\n",
              " 'b': 21,\n",
              " 'u': 22,\n",
              " 'p': 23}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7jXnJ2WRTiu",
        "outputId": "a4f5f932-ba1d-49a6-db23-03ff285167a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded Sequences:\n",
            "[[ 8]\n",
            " [ 2]\n",
            " [11]\n",
            " [ 1]\n",
            " [ 3]\n",
            " [17]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [15]\n",
            " [ 5]\n",
            " [ 2]\n",
            " [17]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 0]\n",
            " [12]\n",
            " [13]\n",
            " [ 0]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 0]\n",
            " [23]\n",
            " [ 2]\n",
            " [ 4]\n",
            " [17]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 7]\n",
            " [ 0]\n",
            " [12]\n",
            " [13]\n",
            " [ 0]\n",
            " [23]\n",
            " [ 5]\n",
            " [ 2]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 0]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [16]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [20]\n",
            " [11]\n",
            " [11]\n",
            " [ 0]\n",
            " [ 7]\n",
            " [10]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [14]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 4]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [21]\n",
            " [ 2]\n",
            " [ 4]\n",
            " [ 4]\n",
            " [ 1]\n",
            " [10]\n",
            " [ 0]\n",
            " [ 1]\n",
            " [ 3]\n",
            " [ 7]\n",
            " [ 0]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 0]\n",
            " [ 0]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 6]\n",
            " [10]\n",
            " [ 0]\n",
            " [12]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [16]\n",
            " [ 0]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [20]\n",
            " [11]\n",
            " [11]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 6]\n",
            " [14]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 6]\n",
            " [11]\n",
            " [11]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 7]\n",
            " [ 6]\n",
            " [10]\n",
            " [14]\n",
            " [ 3]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 0]\n",
            " [21]\n",
            " [13]\n",
            " [ 0]\n",
            " [12]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [11]\n",
            " [18]\n",
            " [ 0]\n",
            " [17]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [20]\n",
            " [12]\n",
            " [ 0]\n",
            " [10]\n",
            " [ 1]\n",
            " [ 6]\n",
            " [ 7]\n",
            " [13]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 5]\n",
            " [ 0]\n",
            " [19]\n",
            " [ 2]\n",
            " [15]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [16]\n",
            " [ 9]\n",
            " [ 6]\n",
            " [ 4]\n",
            " [ 1]\n",
            " [15]\n",
            " [ 1]\n",
            " [10]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 4]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 6]\n",
            " [14]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 0]\n",
            " [ 8]\n",
            " [ 4]\n",
            " [ 6]\n",
            " [ 3]\n",
            " [ 7]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [19]\n",
            " [ 0]\n",
            " [ 6]\n",
            " [ 4]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 1]\n",
            " [ 7]\n",
            " [19]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 5]\n",
            " [18]\n",
            " [ 0]\n",
            " [ 4]\n",
            " [ 9]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [18]\n",
            " [ 2]\n",
            " [10]\n",
            " [ 1]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 0]\n",
            " [ 7]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [20]\n",
            " [ 4]\n",
            " [ 0]\n",
            " [14]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [16]\n",
            " [ 0]\n",
            " [ 9]\n",
            " [ 5]\n",
            " [16]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 0]\n",
            " [ 1]\n",
            " [15]\n",
            " [ 1]\n",
            " [10]\n",
            " [ 0]\n",
            " [ 8]\n",
            " [22]\n",
            " [10]\n",
            " [15]\n",
            " [ 2]\n",
            " [15]\n",
            " [ 1]\n",
            " [ 7]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 0]\n",
            " [18]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [11]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 4]\n",
            " [ 0]\n",
            " [21]\n",
            " [22]\n",
            " [10]\n",
            " [ 3]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [19]\n",
            " [ 0]\n",
            " [22]\n",
            " [ 3]\n",
            " [ 7]\n",
            " [ 1]\n",
            " [10]\n",
            " [ 0]\n",
            " [12]\n",
            " [13]\n",
            " [ 0]\n",
            " [ 8]\n",
            " [14]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 0]\n",
            " [ 6]\n",
            " [ 3]\n",
            " [ 7]\n",
            " [ 0]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [16]\n",
            " [ 0]\n",
            " [ 2]\n",
            " [ 0]\n",
            " [12]\n",
            " [ 0]\n",
            " [21]\n",
            " [ 6]\n",
            " [17]\n",
            " [14]\n",
            " [ 0]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 0]\n",
            " [12]\n",
            " [13]\n",
            " [ 0]\n",
            " [18]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 4]\n",
            " [ 0]\n",
            " [ 6]\n",
            " [19]\n",
            " [ 6]\n",
            " [ 2]\n",
            " [ 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XkGkRnktSD_9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm-NPkSyV4C-",
        "outputId": "394ec705-0487-485f-c393-35f34f90fdb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents = sent_tokenize(text)\n",
        "print(\"Sentence tokens : \",sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw_KwdQcV9Pe",
        "outputId": "3b398bf6-0c4e-49ee-81cd-97d972a67664"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokens :  [\"silence the voices in my head I pitched my poison now I'll drink it to the bitter end heyooo, hear me now, I'll take all the darkness by myself cos I'm ready to give whatever it takes standing at the edge of the fire I don't know how I ever survived I feel it burning under my skin and now I m back on my feet again\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N-JZ3wnWOdf",
        "outputId": "67545550-31ef-424b-b0bb-6f5f09331d2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_text = text.lower()\n",
        "print(lower_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR7hA7MnWU6T",
        "outputId": "4875accd-371a-4f87-ae13-fce947ea9404"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "silence the voices in my head i pitched my poison now i'll drink it to the bitter end heyooo, hear me now, i'll take all the darkness by myself cos i'm ready to give whatever it takes standing at the edge of the fire i don't know how i ever survived i feel it burning under my skin and now i m back on my feet again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing punctuations"
      ],
      "metadata": {
        "id": "YaGSzKBiWnzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "no_pun = text.translate(str.maketrans('', '',string.punctuation))\n",
        "print(\"Without punctuations : \",no_pun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgY5f6S1WiUU",
        "outputId": "07d5482f-cc54-4817-eec3-c76b59f79a2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without punctuations :  silence the voices in my head I pitched my poison now Ill drink it to the bitter end heyooo hear me now Ill take all the darkness by myself cos Im ready to give whatever it takes standing at the edge of the fire I dont know how I ever survived I feel it burning under my skin and now I m back on my feet again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_zReqINWtpF",
        "outputId": "ffa5b857-2b57-4c45-d261-aef00cf3bd7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuDxIc3pXaqk",
        "outputId": "7ca7daf0-da6a-4940-a99d-41f7310fa566"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['silence', 'voices', 'head', 'pitched', 'poison', \"'ll\", 'drink', 'bitter', 'end', 'heyooo', ',', 'hear', ',', \"'ll\", 'take', 'darkness', 'cos', \"'m\", 'ready', 'give', 'whatever', 'takes', 'standing', 'edge', 'fire', \"n't\", 'know', 'ever', 'survived', 'feel', 'burning', 'skin', 'back', 'feet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming and Lemmatization**\n",
        "\n",
        "Stemming reduces words to their root form, while lemmatization reduces words to their base form considering the context."
      ],
      "metadata": {
        "id": "MFRnShqSxGzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "WgL3OjRdYQJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e0c825-4957-4da0-bc9a-e1f6ce1dd49f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stem_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "print(\"Stemmed words : \",stem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd_a6vHExOzT",
        "outputId": "7b6fde4a-d326-4875-a71a-5636a1b7d600"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words :  ['silenc', 'the', 'voic', 'in', 'my', 'head', 'i', 'pitch', 'my', 'poison', 'now', 'i', \"'ll\", 'drink', 'it', 'to', 'the', 'bitter', 'end', 'heyooo', ',', 'hear', 'me', 'now', ',', 'i', \"'ll\", 'take', 'all', 'the', 'dark', 'by', 'myself', 'co', 'i', \"'m\", 'readi', 'to', 'give', 'whatev', 'it', 'take', 'stand', 'at', 'the', 'edg', 'of', 'the', 'fire', 'i', 'do', \"n't\", 'know', 'how', 'i', 'ever', 'surviv', 'i', 'feel', 'it', 'burn', 'under', 'my', 'skin', 'and', 'now', 'i', 'm', 'back', 'on', 'my', 'feet', 'again']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lem_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
        "print(\"Lemmatized words : \",lem_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTbdKXa2xx_w",
        "outputId": "f7aac667-d94a-49be-d5b9-d4205fc6838d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words :  ['silence', 'the', 'voice', 'in', 'my', 'head', 'I', 'pitched', 'my', 'poison', 'now', 'I', \"'ll\", 'drink', 'it', 'to', 'the', 'bitter', 'end', 'heyooo', ',', 'hear', 'me', 'now', ',', 'I', \"'ll\", 'take', 'all', 'the', 'darkness', 'by', 'myself', 'co', 'I', \"'m\", 'ready', 'to', 'give', 'whatever', 'it', 'take', 'standing', 'at', 'the', 'edge', 'of', 'the', 'fire', 'I', 'do', \"n't\", 'know', 'how', 'I', 'ever', 'survived', 'I', 'feel', 'it', 'burning', 'under', 'my', 'skin', 'and', 'now', 'I', 'm', 'back', 'on', 'my', 'foot', 'again']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # regrex\n",
        "no_sym = re.sub(r'[^A-Za-z\\s]','',text)\n",
        "print(\"Text without special symbols : \",no_sym)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xYVK8--yxqG",
        "outputId": "49456f42-6579-46ab-ba6e-5537e5ba88c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without special symbols :  silence the voices in my head I pitched my poison now Ill drink it to the bitter end heyooo hear me now Ill take all the darkness by myself cos Im ready to give whatever it takes standing at the edge of the fire I dont know how I ever survived I feel it burning under my skin and now I m back on my feet again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Remove special characters and numbers\n",
        "    words = [re.sub(r'[^A-Za-z]', '', word) for word in words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "    return words\n",
        "\n",
        "\n",
        "preprocessed_text = preprocess_text(text)\n",
        "print(\"Preprocessed Text:\", preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx6n2kcozdwu",
        "outputId": "b55a220e-75fa-4ad0-cfd6-bb09a64b5ad0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Text: ['silence', 'voice', 'head', 'pitch', 'poison', 'ill', 'drink', 'bitter', 'end', 'heyooo', 'hear', 'ill', 'take', 'darkness', 'cos', 'im', 'ready', 'give', 'whatever', 'take', 'stand', 'edge', 'fire', 'dont', 'know', 'ever', 'survive', 'feel', 'burn', 'skin', 'back', 'feet']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lb8rULsPztr6"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}